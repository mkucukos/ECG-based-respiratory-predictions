{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86af1b40",
   "metadata": {},
   "source": [
    "# Important Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9ce0f88a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook model_build.ipynb to script\n",
      "[NbConvertApp] Writing 42736 bytes to model_build.py\n"
     ]
    }
   ],
   "source": [
    "# !jupyter nbconvert --to script model_build.ipynb\n",
    "\n",
    "import csv\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import neurokit2 as nk\n",
    "from scipy.signal import butter, filtfilt\n",
    "from scipy.signal import resample\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from scipy.stats import pearsonr, spearmanr, linregress\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "import matplotlib.colors as mcolors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54ea7cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.19.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71dc1063",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1760991168.734105   11042 gpu_device.cc:2430] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 12.0. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.\n",
      "W0000 00:00:1760991168.740665   11042 gpu_device.cc:2430] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 12.0. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.\n",
      "I0000 00:00:1760991168.948262   11042 gpu_device.cc:2019] Created device /device:GPU:0 with 13121 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 5070 Ti, pci bus id: 0000:01:00.0, compute capability: 12.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 11164436025408017048\n",
       " xla_global_id: -1,\n",
       " name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 13758365696\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 16388617600126776170\n",
       " physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 5070 Ti, pci bus id: 0000:01:00.0, compute capability: 12.0\"\n",
       " xla_global_id: 416903419]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.experimental.list_physical_devices('GPU')\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aeef116",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2ceb631c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/e/fantasia_respiration/git_repo/ECG-based-respiratory-predictions/notebooks/../dataset/\n",
      "/mnt/e/fantasia_respiration/git_repo/ECG-based-respiratory-predictions/notebooks/../output/run1_3_inputs_model/\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = '3_inputs_model'\n",
    "RUN = 'run1_' + MODEL_NAME+ '/'\n",
    "\n",
    "CURRENT_DIR = os.getcwd() + '/..'\n",
    "DATA_DIR = CURRENT_DIR + '/dataset/'\n",
    "MASTER_OUTPUT_DIR = CURRENT_DIR + '/output/'\n",
    "OUTPUT_DIR = MASTER_OUTPUT_DIR + RUN\n",
    "print(DATA_DIR)\n",
    "print(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c89181c-1819-4e5d-be3a-2be071eaee62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88abecfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_df[data_df['subject_id'] != 'f2y08']['snr'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c0cc7b",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5ba074d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before filter: (8339, 17)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>ecg</th>\n",
       "      <th>resp</th>\n",
       "      <th>Mean_Respiration_Rate</th>\n",
       "      <th>Total_Area_Peak_to_Trough</th>\n",
       "      <th>Total_Area_Trough_to_Peak</th>\n",
       "      <th>Mean_latency_Trough_to_Peak</th>\n",
       "      <th>Mean_latency_Peak_to_Trough</th>\n",
       "      <th>hr_min</th>\n",
       "      <th>hr_mean</th>\n",
       "      <th>hr_max</th>\n",
       "      <th>hrv</th>\n",
       "      <th>snr</th>\n",
       "      <th>Total_peaks</th>\n",
       "      <th>Total_trough</th>\n",
       "      <th>Breath_rate_variability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f1o01</td>\n",
       "      <td>0</td>\n",
       "      <td>[8.46, 8.592, 8.352, 8.152, 8.34, 8.528, 8.312...</td>\n",
       "      <td>[8.488, 8.452, 8.488, 8.496, 8.508, 8.476, 8.4...</td>\n",
       "      <td>14.505123</td>\n",
       "      <td>0.562117</td>\n",
       "      <td>1.169217</td>\n",
       "      <td>1.690667</td>\n",
       "      <td>2.544000</td>\n",
       "      <td>59.055118</td>\n",
       "      <td>62.032517</td>\n",
       "      <td>66.371681</td>\n",
       "      <td>16.625697</td>\n",
       "      <td>5.385646</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>310.321205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f1o01</td>\n",
       "      <td>1</td>\n",
       "      <td>[8.316, 8.58, 8.664, 8.428, 8.264, 8.444, 8.58...</td>\n",
       "      <td>[8.116, 8.124, 8.092, 8.088, 8.06, 8.064, 8.07...</td>\n",
       "      <td>15.780254</td>\n",
       "      <td>1.146417</td>\n",
       "      <td>0.315799</td>\n",
       "      <td>1.525143</td>\n",
       "      <td>2.221333</td>\n",
       "      <td>58.593750</td>\n",
       "      <td>62.245654</td>\n",
       "      <td>66.079295</td>\n",
       "      <td>16.559194</td>\n",
       "      <td>5.984316</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>193.791641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f1o01</td>\n",
       "      <td>2</td>\n",
       "      <td>[8.088, 8.088, 8.26, 8.304, 8.144, 8.072, 8.2,...</td>\n",
       "      <td>[8.656, 8.624, 8.608, 8.596, 8.648, 8.652, 8.6...</td>\n",
       "      <td>13.284661</td>\n",
       "      <td>0.353985</td>\n",
       "      <td>0.445506</td>\n",
       "      <td>1.772000</td>\n",
       "      <td>2.860000</td>\n",
       "      <td>57.692308</td>\n",
       "      <td>59.618603</td>\n",
       "      <td>61.728395</td>\n",
       "      <td>8.315218</td>\n",
       "      <td>6.507062</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>373.388872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f1o01</td>\n",
       "      <td>3</td>\n",
       "      <td>[8.152, 8.02, 8.036, 8.208, 8.208, 8.128, 8.12...</td>\n",
       "      <td>[8.672, 8.684, 8.648, 8.632, 8.628, 8.672, 8.6...</td>\n",
       "      <td>14.913591</td>\n",
       "      <td>0.688938</td>\n",
       "      <td>1.089432</td>\n",
       "      <td>1.680000</td>\n",
       "      <td>2.391000</td>\n",
       "      <td>32.894737</td>\n",
       "      <td>62.955495</td>\n",
       "      <td>69.124424</td>\n",
       "      <td>261.545152</td>\n",
       "      <td>6.893892</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1063.164772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>f1o01</td>\n",
       "      <td>4</td>\n",
       "      <td>[8.068, 8.004, 7.864, 7.9, 8.048, 7.996, 7.848...</td>\n",
       "      <td>[8.548, 8.524, 8.508, 8.528, 8.568, 8.552, 8.5...</td>\n",
       "      <td>16.444328</td>\n",
       "      <td>0.983089</td>\n",
       "      <td>0.837500</td>\n",
       "      <td>1.479200</td>\n",
       "      <td>2.752000</td>\n",
       "      <td>57.251908</td>\n",
       "      <td>62.337664</td>\n",
       "      <td>67.264574</td>\n",
       "      <td>14.326248</td>\n",
       "      <td>5.866205</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1472.451584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9471</th>\n",
       "      <td>f2y10</td>\n",
       "      <td>234</td>\n",
       "      <td>[-0.01708984375, -0.01220703125, -0.01953125, ...</td>\n",
       "      <td>[0.54443359375, 0.5419921875, 0.5517578125, 0....</td>\n",
       "      <td>15.151441</td>\n",
       "      <td>0.219147</td>\n",
       "      <td>0.206771</td>\n",
       "      <td>1.567200</td>\n",
       "      <td>2.387000</td>\n",
       "      <td>49.180328</td>\n",
       "      <td>56.770476</td>\n",
       "      <td>62.761506</td>\n",
       "      <td>542.688370</td>\n",
       "      <td>6.556642</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>169.671251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9472</th>\n",
       "      <td>f2y10</td>\n",
       "      <td>235</td>\n",
       "      <td>[-0.1220703125, -0.23681640625, -0.25390625, -...</td>\n",
       "      <td>[-0.17333984375, -0.1708984375, -0.17333984375...</td>\n",
       "      <td>10.984443</td>\n",
       "      <td>0.493156</td>\n",
       "      <td>0.759228</td>\n",
       "      <td>2.105000</td>\n",
       "      <td>3.357333</td>\n",
       "      <td>54.347826</td>\n",
       "      <td>58.418115</td>\n",
       "      <td>64.935065</td>\n",
       "      <td>47.516079</td>\n",
       "      <td>3.000940</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>125.795469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9473</th>\n",
       "      <td>f2y10</td>\n",
       "      <td>236</td>\n",
       "      <td>[0.1025390625, 0.07568359375, 0.0439453125, 0....</td>\n",
       "      <td>[-0.185546875, -0.185546875, -0.1806640625, -0...</td>\n",
       "      <td>14.860658</td>\n",
       "      <td>0.272106</td>\n",
       "      <td>0.304594</td>\n",
       "      <td>1.684667</td>\n",
       "      <td>2.696000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>56.347270</td>\n",
       "      <td>64.655172</td>\n",
       "      <td>63.884896</td>\n",
       "      <td>7.360403</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>211.406007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9474</th>\n",
       "      <td>f2y10</td>\n",
       "      <td>237</td>\n",
       "      <td>[-0.1123046875, -0.107421875, -0.10986328125, ...</td>\n",
       "      <td>[0.0830078125, 0.087890625, 0.0927734375, 0.09...</td>\n",
       "      <td>11.101286</td>\n",
       "      <td>0.637757</td>\n",
       "      <td>1.059193</td>\n",
       "      <td>2.117000</td>\n",
       "      <td>3.465333</td>\n",
       "      <td>50.335570</td>\n",
       "      <td>58.717126</td>\n",
       "      <td>69.444444</td>\n",
       "      <td>81.028010</td>\n",
       "      <td>3.194175</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>551.026769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9475</th>\n",
       "      <td>f2y10</td>\n",
       "      <td>238</td>\n",
       "      <td>[-0.19287109375, -0.18310546875, -0.1733398437...</td>\n",
       "      <td>[0.30517578125, 0.3076171875, 0.3076171875, 0....</td>\n",
       "      <td>15.198277</td>\n",
       "      <td>0.335339</td>\n",
       "      <td>0.130116</td>\n",
       "      <td>1.591200</td>\n",
       "      <td>2.477000</td>\n",
       "      <td>53.191489</td>\n",
       "      <td>58.333934</td>\n",
       "      <td>64.102564</td>\n",
       "      <td>47.096434</td>\n",
       "      <td>7.639396</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>76.828814</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8339 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     subject_id  chunk_id                                                ecg  \\\n",
       "0         f1o01         0  [8.46, 8.592, 8.352, 8.152, 8.34, 8.528, 8.312...   \n",
       "1         f1o01         1  [8.316, 8.58, 8.664, 8.428, 8.264, 8.444, 8.58...   \n",
       "2         f1o01         2  [8.088, 8.088, 8.26, 8.304, 8.144, 8.072, 8.2,...   \n",
       "3         f1o01         3  [8.152, 8.02, 8.036, 8.208, 8.208, 8.128, 8.12...   \n",
       "4         f1o01         4  [8.068, 8.004, 7.864, 7.9, 8.048, 7.996, 7.848...   \n",
       "...         ...       ...                                                ...   \n",
       "9471      f2y10       234  [-0.01708984375, -0.01220703125, -0.01953125, ...   \n",
       "9472      f2y10       235  [-0.1220703125, -0.23681640625, -0.25390625, -...   \n",
       "9473      f2y10       236  [0.1025390625, 0.07568359375, 0.0439453125, 0....   \n",
       "9474      f2y10       237  [-0.1123046875, -0.107421875, -0.10986328125, ...   \n",
       "9475      f2y10       238  [-0.19287109375, -0.18310546875, -0.1733398437...   \n",
       "\n",
       "                                                   resp  \\\n",
       "0     [8.488, 8.452, 8.488, 8.496, 8.508, 8.476, 8.4...   \n",
       "1     [8.116, 8.124, 8.092, 8.088, 8.06, 8.064, 8.07...   \n",
       "2     [8.656, 8.624, 8.608, 8.596, 8.648, 8.652, 8.6...   \n",
       "3     [8.672, 8.684, 8.648, 8.632, 8.628, 8.672, 8.6...   \n",
       "4     [8.548, 8.524, 8.508, 8.528, 8.568, 8.552, 8.5...   \n",
       "...                                                 ...   \n",
       "9471  [0.54443359375, 0.5419921875, 0.5517578125, 0....   \n",
       "9472  [-0.17333984375, -0.1708984375, -0.17333984375...   \n",
       "9473  [-0.185546875, -0.185546875, -0.1806640625, -0...   \n",
       "9474  [0.0830078125, 0.087890625, 0.0927734375, 0.09...   \n",
       "9475  [0.30517578125, 0.3076171875, 0.3076171875, 0....   \n",
       "\n",
       "      Mean_Respiration_Rate  Total_Area_Peak_to_Trough  \\\n",
       "0                 14.505123                   0.562117   \n",
       "1                 15.780254                   1.146417   \n",
       "2                 13.284661                   0.353985   \n",
       "3                 14.913591                   0.688938   \n",
       "4                 16.444328                   0.983089   \n",
       "...                     ...                        ...   \n",
       "9471              15.151441                   0.219147   \n",
       "9472              10.984443                   0.493156   \n",
       "9473              14.860658                   0.272106   \n",
       "9474              11.101286                   0.637757   \n",
       "9475              15.198277                   0.335339   \n",
       "\n",
       "      Total_Area_Trough_to_Peak  Mean_latency_Trough_to_Peak  \\\n",
       "0                      1.169217                     1.690667   \n",
       "1                      0.315799                     1.525143   \n",
       "2                      0.445506                     1.772000   \n",
       "3                      1.089432                     1.680000   \n",
       "4                      0.837500                     1.479200   \n",
       "...                         ...                          ...   \n",
       "9471                   0.206771                     1.567200   \n",
       "9472                   0.759228                     2.105000   \n",
       "9473                   0.304594                     1.684667   \n",
       "9474                   1.059193                     2.117000   \n",
       "9475                   0.130116                     1.591200   \n",
       "\n",
       "      Mean_latency_Peak_to_Trough     hr_min    hr_mean     hr_max  \\\n",
       "0                        2.544000  59.055118  62.032517  66.371681   \n",
       "1                        2.221333  58.593750  62.245654  66.079295   \n",
       "2                        2.860000  57.692308  59.618603  61.728395   \n",
       "3                        2.391000  32.894737  62.955495  69.124424   \n",
       "4                        2.752000  57.251908  62.337664  67.264574   \n",
       "...                           ...        ...        ...        ...   \n",
       "9471                     2.387000  49.180328  56.770476  62.761506   \n",
       "9472                     3.357333  54.347826  58.418115  64.935065   \n",
       "9473                     2.696000  50.000000  56.347270  64.655172   \n",
       "9474                     3.465333  50.335570  58.717126  69.444444   \n",
       "9475                     2.477000  53.191489  58.333934  64.102564   \n",
       "\n",
       "             hrv       snr  Total_peaks  Total_trough  Breath_rate_variability  \n",
       "0      16.625697  5.385646          6.0           6.0               310.321205  \n",
       "1      16.559194  5.984316          7.0           7.0               193.791641  \n",
       "2       8.315218  6.507062          6.0           6.0               373.388872  \n",
       "3     261.545152  6.893892          5.0           5.0              1063.164772  \n",
       "4      14.326248  5.866205          5.0           5.0              1472.451584  \n",
       "...          ...       ...          ...           ...                      ...  \n",
       "9471  542.688370  6.556642          5.0           5.0               169.671251  \n",
       "9472   47.516079  3.000940          4.0           4.0               125.795469  \n",
       "9473   63.884896  7.360403          6.0           6.0               211.406007  \n",
       "9474   81.028010  3.194175          4.0           4.0               551.026769  \n",
       "9475   47.096434  7.639396          5.0           5.0                76.828814  \n",
       "\n",
       "[8339 rows x 17 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df = pd.read_pickle(DATA_DIR + 'fantasia_dataset_preprocessing.plk')\n",
    "print(\"Before filter:\", data_df.shape)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b086b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After filter: (8105, 17)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>ecg</th>\n",
       "      <th>resp</th>\n",
       "      <th>Mean_Respiration_Rate</th>\n",
       "      <th>Total_Area_Peak_to_Trough</th>\n",
       "      <th>Total_Area_Trough_to_Peak</th>\n",
       "      <th>Mean_latency_Trough_to_Peak</th>\n",
       "      <th>Mean_latency_Peak_to_Trough</th>\n",
       "      <th>hr_min</th>\n",
       "      <th>hr_mean</th>\n",
       "      <th>hr_max</th>\n",
       "      <th>hrv</th>\n",
       "      <th>snr</th>\n",
       "      <th>Total_peaks</th>\n",
       "      <th>Total_trough</th>\n",
       "      <th>Breath_rate_variability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f1o01</td>\n",
       "      <td>0</td>\n",
       "      <td>[8.46, 8.592, 8.352, 8.152, 8.34, 8.528, 8.312...</td>\n",
       "      <td>[8.488, 8.452, 8.488, 8.496, 8.508, 8.476, 8.4...</td>\n",
       "      <td>14.505123</td>\n",
       "      <td>0.562117</td>\n",
       "      <td>1.169217</td>\n",
       "      <td>1.690667</td>\n",
       "      <td>2.544000</td>\n",
       "      <td>59.055118</td>\n",
       "      <td>62.032517</td>\n",
       "      <td>66.371681</td>\n",
       "      <td>16.625697</td>\n",
       "      <td>5.385646</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>310.321205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f1o01</td>\n",
       "      <td>1</td>\n",
       "      <td>[8.316, 8.58, 8.664, 8.428, 8.264, 8.444, 8.58...</td>\n",
       "      <td>[8.116, 8.124, 8.092, 8.088, 8.06, 8.064, 8.07...</td>\n",
       "      <td>15.780254</td>\n",
       "      <td>1.146417</td>\n",
       "      <td>0.315799</td>\n",
       "      <td>1.525143</td>\n",
       "      <td>2.221333</td>\n",
       "      <td>58.593750</td>\n",
       "      <td>62.245654</td>\n",
       "      <td>66.079295</td>\n",
       "      <td>16.559194</td>\n",
       "      <td>5.984316</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>193.791641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f1o01</td>\n",
       "      <td>2</td>\n",
       "      <td>[8.088, 8.088, 8.26, 8.304, 8.144, 8.072, 8.2,...</td>\n",
       "      <td>[8.656, 8.624, 8.608, 8.596, 8.648, 8.652, 8.6...</td>\n",
       "      <td>13.284661</td>\n",
       "      <td>0.353985</td>\n",
       "      <td>0.445506</td>\n",
       "      <td>1.772000</td>\n",
       "      <td>2.860000</td>\n",
       "      <td>57.692308</td>\n",
       "      <td>59.618603</td>\n",
       "      <td>61.728395</td>\n",
       "      <td>8.315218</td>\n",
       "      <td>6.507062</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>373.388872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f1o01</td>\n",
       "      <td>3</td>\n",
       "      <td>[8.152, 8.02, 8.036, 8.208, 8.208, 8.128, 8.12...</td>\n",
       "      <td>[8.672, 8.684, 8.648, 8.632, 8.628, 8.672, 8.6...</td>\n",
       "      <td>14.913591</td>\n",
       "      <td>0.688938</td>\n",
       "      <td>1.089432</td>\n",
       "      <td>1.680000</td>\n",
       "      <td>2.391000</td>\n",
       "      <td>32.894737</td>\n",
       "      <td>62.955495</td>\n",
       "      <td>69.124424</td>\n",
       "      <td>261.545152</td>\n",
       "      <td>6.893892</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1063.164772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>f1o01</td>\n",
       "      <td>4</td>\n",
       "      <td>[8.068, 8.004, 7.864, 7.9, 8.048, 7.996, 7.848...</td>\n",
       "      <td>[8.548, 8.524, 8.508, 8.528, 8.568, 8.552, 8.5...</td>\n",
       "      <td>16.444328</td>\n",
       "      <td>0.983089</td>\n",
       "      <td>0.837500</td>\n",
       "      <td>1.479200</td>\n",
       "      <td>2.752000</td>\n",
       "      <td>57.251908</td>\n",
       "      <td>62.337664</td>\n",
       "      <td>67.264574</td>\n",
       "      <td>14.326248</td>\n",
       "      <td>5.866205</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1472.451584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9469</th>\n",
       "      <td>f2y10</td>\n",
       "      <td>232</td>\n",
       "      <td>[-0.10009765625, -0.09765625, -0.10009765625, ...</td>\n",
       "      <td>[0.0048828125, 0.0048828125, 0.0048828125, 0.0...</td>\n",
       "      <td>14.497019</td>\n",
       "      <td>0.197665</td>\n",
       "      <td>0.101241</td>\n",
       "      <td>1.556800</td>\n",
       "      <td>2.480000</td>\n",
       "      <td>52.631579</td>\n",
       "      <td>58.396608</td>\n",
       "      <td>66.079295</td>\n",
       "      <td>66.171494</td>\n",
       "      <td>6.985175</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>67.014924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9470</th>\n",
       "      <td>f2y10</td>\n",
       "      <td>233</td>\n",
       "      <td>[-0.185546875, -0.1806640625, -0.18310546875, ...</td>\n",
       "      <td>[-0.126953125, -0.12939453125, -0.126953125, -...</td>\n",
       "      <td>12.471924</td>\n",
       "      <td>0.304567</td>\n",
       "      <td>0.516003</td>\n",
       "      <td>1.549600</td>\n",
       "      <td>3.345000</td>\n",
       "      <td>51.369863</td>\n",
       "      <td>58.250585</td>\n",
       "      <td>65.789474</td>\n",
       "      <td>56.823614</td>\n",
       "      <td>7.723637</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>176.674843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9471</th>\n",
       "      <td>f2y10</td>\n",
       "      <td>234</td>\n",
       "      <td>[-0.01708984375, -0.01220703125, -0.01953125, ...</td>\n",
       "      <td>[0.54443359375, 0.5419921875, 0.5517578125, 0....</td>\n",
       "      <td>15.151441</td>\n",
       "      <td>0.219147</td>\n",
       "      <td>0.206771</td>\n",
       "      <td>1.567200</td>\n",
       "      <td>2.387000</td>\n",
       "      <td>49.180328</td>\n",
       "      <td>56.770476</td>\n",
       "      <td>62.761506</td>\n",
       "      <td>542.688370</td>\n",
       "      <td>6.556642</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>169.671251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9473</th>\n",
       "      <td>f2y10</td>\n",
       "      <td>236</td>\n",
       "      <td>[0.1025390625, 0.07568359375, 0.0439453125, 0....</td>\n",
       "      <td>[-0.185546875, -0.185546875, -0.1806640625, -0...</td>\n",
       "      <td>14.860658</td>\n",
       "      <td>0.272106</td>\n",
       "      <td>0.304594</td>\n",
       "      <td>1.684667</td>\n",
       "      <td>2.696000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>56.347270</td>\n",
       "      <td>64.655172</td>\n",
       "      <td>63.884896</td>\n",
       "      <td>7.360403</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>211.406007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9475</th>\n",
       "      <td>f2y10</td>\n",
       "      <td>238</td>\n",
       "      <td>[-0.19287109375, -0.18310546875, -0.1733398437...</td>\n",
       "      <td>[0.30517578125, 0.3076171875, 0.3076171875, 0....</td>\n",
       "      <td>15.198277</td>\n",
       "      <td>0.335339</td>\n",
       "      <td>0.130116</td>\n",
       "      <td>1.591200</td>\n",
       "      <td>2.477000</td>\n",
       "      <td>53.191489</td>\n",
       "      <td>58.333934</td>\n",
       "      <td>64.102564</td>\n",
       "      <td>47.096434</td>\n",
       "      <td>7.639396</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>76.828814</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8105 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     subject_id  chunk_id                                                ecg  \\\n",
       "0         f1o01         0  [8.46, 8.592, 8.352, 8.152, 8.34, 8.528, 8.312...   \n",
       "1         f1o01         1  [8.316, 8.58, 8.664, 8.428, 8.264, 8.444, 8.58...   \n",
       "2         f1o01         2  [8.088, 8.088, 8.26, 8.304, 8.144, 8.072, 8.2,...   \n",
       "3         f1o01         3  [8.152, 8.02, 8.036, 8.208, 8.208, 8.128, 8.12...   \n",
       "4         f1o01         4  [8.068, 8.004, 7.864, 7.9, 8.048, 7.996, 7.848...   \n",
       "...         ...       ...                                                ...   \n",
       "9469      f2y10       232  [-0.10009765625, -0.09765625, -0.10009765625, ...   \n",
       "9470      f2y10       233  [-0.185546875, -0.1806640625, -0.18310546875, ...   \n",
       "9471      f2y10       234  [-0.01708984375, -0.01220703125, -0.01953125, ...   \n",
       "9473      f2y10       236  [0.1025390625, 0.07568359375, 0.0439453125, 0....   \n",
       "9475      f2y10       238  [-0.19287109375, -0.18310546875, -0.1733398437...   \n",
       "\n",
       "                                                   resp  \\\n",
       "0     [8.488, 8.452, 8.488, 8.496, 8.508, 8.476, 8.4...   \n",
       "1     [8.116, 8.124, 8.092, 8.088, 8.06, 8.064, 8.07...   \n",
       "2     [8.656, 8.624, 8.608, 8.596, 8.648, 8.652, 8.6...   \n",
       "3     [8.672, 8.684, 8.648, 8.632, 8.628, 8.672, 8.6...   \n",
       "4     [8.548, 8.524, 8.508, 8.528, 8.568, 8.552, 8.5...   \n",
       "...                                                 ...   \n",
       "9469  [0.0048828125, 0.0048828125, 0.0048828125, 0.0...   \n",
       "9470  [-0.126953125, -0.12939453125, -0.126953125, -...   \n",
       "9471  [0.54443359375, 0.5419921875, 0.5517578125, 0....   \n",
       "9473  [-0.185546875, -0.185546875, -0.1806640625, -0...   \n",
       "9475  [0.30517578125, 0.3076171875, 0.3076171875, 0....   \n",
       "\n",
       "      Mean_Respiration_Rate  Total_Area_Peak_to_Trough  \\\n",
       "0                 14.505123                   0.562117   \n",
       "1                 15.780254                   1.146417   \n",
       "2                 13.284661                   0.353985   \n",
       "3                 14.913591                   0.688938   \n",
       "4                 16.444328                   0.983089   \n",
       "...                     ...                        ...   \n",
       "9469              14.497019                   0.197665   \n",
       "9470              12.471924                   0.304567   \n",
       "9471              15.151441                   0.219147   \n",
       "9473              14.860658                   0.272106   \n",
       "9475              15.198277                   0.335339   \n",
       "\n",
       "      Total_Area_Trough_to_Peak  Mean_latency_Trough_to_Peak  \\\n",
       "0                      1.169217                     1.690667   \n",
       "1                      0.315799                     1.525143   \n",
       "2                      0.445506                     1.772000   \n",
       "3                      1.089432                     1.680000   \n",
       "4                      0.837500                     1.479200   \n",
       "...                         ...                          ...   \n",
       "9469                   0.101241                     1.556800   \n",
       "9470                   0.516003                     1.549600   \n",
       "9471                   0.206771                     1.567200   \n",
       "9473                   0.304594                     1.684667   \n",
       "9475                   0.130116                     1.591200   \n",
       "\n",
       "      Mean_latency_Peak_to_Trough     hr_min    hr_mean     hr_max  \\\n",
       "0                        2.544000  59.055118  62.032517  66.371681   \n",
       "1                        2.221333  58.593750  62.245654  66.079295   \n",
       "2                        2.860000  57.692308  59.618603  61.728395   \n",
       "3                        2.391000  32.894737  62.955495  69.124424   \n",
       "4                        2.752000  57.251908  62.337664  67.264574   \n",
       "...                           ...        ...        ...        ...   \n",
       "9469                     2.480000  52.631579  58.396608  66.079295   \n",
       "9470                     3.345000  51.369863  58.250585  65.789474   \n",
       "9471                     2.387000  49.180328  56.770476  62.761506   \n",
       "9473                     2.696000  50.000000  56.347270  64.655172   \n",
       "9475                     2.477000  53.191489  58.333934  64.102564   \n",
       "\n",
       "             hrv       snr  Total_peaks  Total_trough  Breath_rate_variability  \n",
       "0      16.625697  5.385646          6.0           6.0               310.321205  \n",
       "1      16.559194  5.984316          7.0           7.0               193.791641  \n",
       "2       8.315218  6.507062          6.0           6.0               373.388872  \n",
       "3     261.545152  6.893892          5.0           5.0              1063.164772  \n",
       "4      14.326248  5.866205          5.0           5.0              1472.451584  \n",
       "...          ...       ...          ...           ...                      ...  \n",
       "9469   66.171494  6.985175          5.0           5.0                67.014924  \n",
       "9470   56.823614  7.723637          5.0           5.0               176.674843  \n",
       "9471  542.688370  6.556642          5.0           5.0               169.671251  \n",
       "9473   63.884896  7.360403          6.0           6.0               211.406007  \n",
       "9475   47.096434  7.639396          5.0           5.0                76.828814  \n",
       "\n",
       "[8105 rows x 17 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df = data_df[(data_df['snr'] >= 4) & (data_df['Breath_rate_variability'] <= 1700)]\n",
    "print(\"After filter:\", data_df.shape)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe08e57b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cda25aa2",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afebc31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_df = data_df #allias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e2420f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_df = chunks_df.dropna(subset=['Mean_Respiration_Rate','hrv'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9b23cb",
   "metadata": {},
   "source": [
    "## Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb41e339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'ecg' column to a 3D numpy array\n",
    "X = np.array(chunks_df['ecg'].tolist())\n",
    "Y = chunks_df[['Mean_Respiration_Rate','Total_Area_Trough_to_Peak','Total_Area_Peak_to_Trough',\n",
    "            'Mean_latency_Trough_to_Peak','Mean_latency_Peak_to_Trough']].values\n",
    "\n",
    "# Normalize each ECG chunk individually and handle NaNs\n",
    "X_normalized = np.zeros_like(X)\n",
    "for i in range(X.shape[0]):\n",
    "    mean = np.mean(X[i])\n",
    "    std = np.std(X[i])\n",
    "    if std == 0:  # Avoid division by zero\n",
    "        print('sid == 0')\n",
    "        std = 1\n",
    "    X_normalized[i] = (X[i] - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c9eb22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_feature = np.array(chunks_df[['hr_min','hr_mean','hr_max','hrv','snr']])\n",
    "\n",
    "\n",
    "# Normalize each ECG chunk individually and handle NaNs\n",
    "X_feature_normalized = np.zeros_like(X_feature)\n",
    "for i in range(X_feature.shape[1]):\n",
    "    mean = np.mean(X_feature[:,i])\n",
    "    std = np.std(X_feature[:,i])\n",
    "    if std == 0:  # Avoid division by zero\n",
    "        print('sid == 0')\n",
    "        std = 1\n",
    "    X_feature_normalized[:,i] = (X_feature[:,i] - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b00bffc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8099, 7500)\n",
      "(8099, 5)\n",
      "(8099, 5)\n"
     ]
    }
   ],
   "source": [
    "print(X_normalized.shape)\n",
    "print(X_feature_normalized.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75887b81-4fcc-4f1b-81f0-266eec9c7273",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0f5209c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "NaN in X_normalized after cleaning: 0, Inf in X_normalized: 0\n"
     ]
    }
   ],
   "source": [
    "# Remove Nan value\n",
    "nan_rows = np.isnan(X_normalized).any(axis=1)\n",
    "\n",
    "# Remove rows with NaNs\n",
    "X_normalized = X_normalized[~nan_rows]\n",
    "X_feature_normalized = X_feature_normalized[~nan_rows]\n",
    "Y = Y[~nan_rows]\n",
    "cleaned_chunks_df = chunks_df[~nan_rows]\n",
    "\n",
    "print(nan_rows.sum())\n",
    "print(f\"NaN in X_normalized after cleaning: {np.isnan(X_normalized).sum()}, Inf in X_normalized: {np.isinf(X_normalized).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c02923b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8099, 7500)\n",
      "(8099, 5)\n",
      "(8099, 5)\n"
     ]
    }
   ],
   "source": [
    "print(X_normalized.shape)\n",
    "print(X_feature_normalized.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e71795fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate the 2.5th and 97.5th percentiles\n",
    "# for i in range(Y.shape[1]):\n",
    "#     lower_percentile = np.percentile(Y[:,i], 0.5)\n",
    "#     upper_percentile = np.percentile(Y[:,i], 99.5)\n",
    "\n",
    "\n",
    "\n",
    "#     # Verify the clipping\n",
    "#     print('\\n', i)\n",
    "#     print(f\"1th percentile: {lower_percentile}\")\n",
    "#     print(f\"99th percentile: {upper_percentile}\")\n",
    "#     print(f\"Before clipping: min = {Y[:,i].min()}, max = {Y[:,i].max()}\")\n",
    "#     # Clip the values in y_train\n",
    "#     Y[:,i] = np.clip(Y[:,i], lower_percentile, upper_percentile)\n",
    "#     print(f\"After clipping: min = {Y[:,i].min()}, max = {Y[:,i].max()}\")\n",
    "\n",
    "#     # Continue with training the model using y_train_clipped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b40236",
   "metadata": {},
   "source": [
    "Spliting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eade0e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "subjects = cleaned_chunks_df['subject_id'].unique()\n",
    "train_val_subjects, test_subjects = train_test_split(subjects, test_size=0.2, random_state=96)\n",
    "train_subjects, val_subjects = train_test_split(train_val_subjects, test_size=0.15, random_state=96)\n",
    "\n",
    "# Create boolean masks for train, validation, and test subjects\n",
    "train_mask = cleaned_chunks_df['subject_id'].isin(train_subjects)\n",
    "val_mask = cleaned_chunks_df['subject_id'].isin(val_subjects)\n",
    "test_mask = cleaned_chunks_df['subject_id'].isin(test_subjects)\n",
    "\n",
    "# Use these masks to create the actual train, validation, and test datasets\n",
    "train_data = cleaned_chunks_df[train_mask]\n",
    "val_data = cleaned_chunks_df[val_mask]\n",
    "test_data = cleaned_chunks_df[test_mask]\n",
    "\n",
    "# Now you have train, validation, and test datasets based on the subjects\n",
    "test_subject_id = cleaned_chunks_df[test_mask]['subject_id'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "387f3dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['f1o06' 'f1y09' 'f1o03' 'f2o02' 'f2o05' 'f1o09' 'f1o08' 'f2y10' 'f1o10'\n",
      " 'f2o04' 'f2y08' 'f1o05' 'f1o04' 'f1y01' 'f2y06' 'f2y07' 'f1o02' 'f2y04'\n",
      " 'f2o09' 'f1y05' 'f2o07' 'f2o10' 'f2o01' 'f1y06' 'f2o03' 'f1o07' 'f2y09']\n",
      "['f1y08' 'f1y04' 'f1y03' 'f2y02' 'f2y01']\n",
      "['f1y10' 'f1o01' 'f1y07' 'f2o06' 'f2y03' 'f1y02' 'f2o08' 'f2y05']\n"
     ]
    }
   ],
   "source": [
    "print(train_subjects)\n",
    "print(val_subjects)\n",
    "print(test_subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "50b45386-8872-452a-86c5-294edab64b52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00809941,  0.40926564, -0.34957991, ...,  0.27014396,\n",
       "         0.40926564, -0.26104793],\n",
       "       [ 0.26904106,  1.15557631,  1.43765571, ..., -0.33541479,\n",
       "         0.17501459,  0.17501459],\n",
       "       [-0.7231427 , -0.7231427 , -0.09334483, ..., -0.98677902,\n",
       "        -0.64991039, -0.07869837],\n",
       "       ...,\n",
       "       [-0.02289313,  0.00324655, -0.03596296, ...,  2.7609822 ,\n",
       "         1.85916343,  0.65673841],\n",
       "       [ 0.90269696,  0.74909981,  0.56757591, ..., -0.27022672,\n",
       "        -0.34004361, -0.34004361],\n",
       "       [-0.79813056, -0.73697144, -0.67581233, ..., -0.46175542,\n",
       "        -0.37001674, -0.33943718]], shape=(8099, 7500))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6094844d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (5445, 7500, 1)\n",
      "X_val shape: (1021, 7500, 1)\n",
      "X_test shape: (1633, 7500, 1)\n",
      "y_train shape: (5445, 5)\n",
      "y_val shape: (1021, 5)\n",
      "y_test shape: (1633, 5)\n",
      "test_subject_id shape: (1633,)\n"
     ]
    }
   ],
   "source": [
    "# Apply the masks to create train, validation, and test sets\n",
    "X_train = np.expand_dims(X_normalized[train_mask], 2)\n",
    "Y_train = Y[train_mask] \n",
    "X_val = np.expand_dims(X_normalized[val_mask], 2)\n",
    "Y_val = Y[val_mask]\n",
    "X_test = np.expand_dims(X_normalized[test_mask], 2)\n",
    "Y_test = Y[test_mask]\n",
    "\n",
    "# Print shapes of the datasets to verify\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {Y_train.shape}\")\n",
    "print(f\"y_val shape: {Y_val.shape}\")\n",
    "print(f\"y_test shape: {Y_test.shape}\")\n",
    "print(f\"test_subject_id shape: {test_subject_id.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1d9a12e0-b75f-4de7-91cc-4952b27cb601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_feature shape: (5445, 5)\n",
      "X_val_feature shape: (1021, 5)\n",
      "X_test_feature shape: (1633, 5)\n"
     ]
    }
   ],
   "source": [
    "# Apply the masks to create train, validation, and test sets\n",
    "X_train_feature = X_feature_normalized[train_mask]\n",
    "X_val_feature = X_feature_normalized[val_mask]\n",
    "X_test_feature = X_feature_normalized[test_mask]\n",
    "\n",
    "\n",
    "# Print shapes of the datasets to verify\n",
    "print(f\"X_train_feature shape: {X_train_feature.shape}\")\n",
    "print(f\"X_val_feature shape: {X_val_feature.shape}\")\n",
    "print(f\"X_test_feature shape: {X_test_feature.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "085209bb-3abd-4429-a551-76540f5fea98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_freq_domain(X, SAMPLE_RATE = 250, N_SAMPLES = 30*250):\n",
    "    X_freq = []\n",
    "    for i in range(X.shape[0]):\n",
    "        data = X_train[i]\n",
    "        frequency_domain = np.fft.fft(data)\n",
    "        #frequencies = np.fft.fftfreq(N_SAMPLES, 1/SAMPLE_RATE)\n",
    "        X_freq.append(np.abs(frequency_domain))\n",
    "    X_freq = np.array(X_freq)\n",
    "    return X_freq\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d6e383c0-620e-449b-b620-38af89c2b7cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_freq shape: (5445, 7500, 1)\n",
      "X_val_freq shape: (1021, 7500, 1)\n",
      "X_test_freq shape: (1633, 7500, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train_freq = compute_freq_domain(X_train)\n",
    "X_val_freq =  compute_freq_domain(X_val)\n",
    "X_test_freq = compute_freq_domain(X_test)\n",
    "\n",
    "print(f\"X_train_freq shape: {X_train_freq.shape}\")\n",
    "print(f\"X_val_freq shape: {X_val_freq.shape}\")\n",
    "print(f\"X_test_freq shape: {X_test_freq.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "214f7654-6a3d-4826-b891-4b5dc3e05b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pywt\n",
    "\n",
    "def extract_wavelet_features(X, wavelet='db4', level=4):\n",
    "    n_samples = X.shape[0]\n",
    "    \n",
    "    #calculate n_feature\n",
    "    n_features = 0\n",
    "    temp = pywt.wavedec(np.zeros(X.shape[1]), wavelet, level=level)\n",
    "    for i in temp:\n",
    "        n_features += len(i)\n",
    "\n",
    "    X_features = np.zeros((n_samples, n_features))\n",
    "    for i in range(n_samples):\n",
    "        current_signal = X[i, :, 0]\n",
    "        \n",
    "        coeffs = pywt.wavedec(current_signal, wavelet, level=level)\n",
    "        features = np.concatenate(coeffs)\n",
    "        \n",
    "        X_features[i, :] = features\n",
    "    return X_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1e232d89-bced-4113-968a-5a5eba6ab0b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_wavelet shape: (5445, 7526)\n",
      "X_val_wavelet shape: (1021, 7526)\n",
      "X_test_wavelet shape: (1633, 7526)\n"
     ]
    }
   ],
   "source": [
    "X_train_wavelet = extract_wavelet_features(X_train, wavelet='db4', level=4)\n",
    "X_val_wavelet =  extract_wavelet_features(X_val, wavelet='db4', level=4)\n",
    "X_test_wavelet = extract_wavelet_features(X_test, wavelet='db4', level=4)\n",
    "\n",
    "print(f\"X_train_wavelet shape: {X_train_wavelet.shape}\")\n",
    "print(f\"X_val_wavelet shape: {X_val_wavelet.shape}\")\n",
    "print(f\"X_test_wavelet shape: {X_test_wavelet.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c75e13b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.05246714 0.15850716 0.12722465 0.41182092 0.24998013]\n"
     ]
    }
   ],
   "source": [
    "output_ranges = np.max(Y_train, axis=0) - np.min(Y_train, axis=0)\n",
    "\n",
    "# Calculate the inverse of the range to use as scaling factors\n",
    "scaling_factors = 1 / output_ranges\n",
    "\n",
    "# Normalize scaling factors to sum up to 1\n",
    "scaling_factors /= scaling_factors.sum()\n",
    "\n",
    "print(scaling_factors) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ad99eb",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a3ddf020",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 200\n",
    "BATCH_SIZE = 64\n",
    "PATIENCE = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "85523f82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['f1o01', 'f1o01', 'f1o01', ..., 'f2y05', 'f2y05', 'f2y05'],\n",
       "      shape=(1633,), dtype=object)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_chunks_df[test_mask]['subject_id'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b215ed76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa80b79-31e6-4b7a-b36f-2621d998c859",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b06e9ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create model\n",
    "def create_CNN_model_5_output(shape, start_neuron = 64, kernel_size=9, strides_size=1, max_pool_size=3, dropout=0.3, padding='valid', delta=1.0):    \n",
    "\n",
    "    input_layer = tf.keras.Input((shape[1], shape[2])) \n",
    "\n",
    "    \n",
    "    conv_1 = tf.keras.layers.Conv1D(start_neuron * 1, kernel_size, strides=strides_size,  padding=padding, activation=tf.keras.layers.LeakyReLU(),kernel_regularizer=tf.keras.regularizers.l2(0.01))(input_layer)\n",
    "    conv_1 = tf.keras.layers.MaxPool1D(max_pool_size,  padding=padding)(conv_1)\n",
    "    conv_1 = tf.keras.layers.Dropout(dropout)(conv_1)\n",
    "    \n",
    "    conv_2 = tf.keras.layers.Conv1D(start_neuron * 2, kernel_size, strides=strides_size, padding=padding, activation=tf.keras.layers.LeakyReLU(),kernel_regularizer=tf.keras.regularizers.l2(0.01))(conv_1)\n",
    "    conv_2 = tf.keras.layers.MaxPool1D(max_pool_size,  padding=padding)(conv_2)\n",
    "    conv_2 = tf.keras.layers.Dropout(dropout)(conv_2)    \n",
    "    \n",
    "    conv_3 = tf.keras.layers.Conv1D(start_neuron * 4, kernel_size, strides=strides_size, padding=padding, activation=tf.keras.layers.LeakyReLU(),kernel_regularizer=tf.keras.regularizers.l2(0.01))(conv_2)\n",
    "    conv_3 = tf.keras.layers.MaxPool1D(max_pool_size,  padding=padding)(conv_3)\n",
    "    conv_3 = tf.keras.layers.Dropout(dropout)(conv_3)\n",
    "    \n",
    "    \n",
    "    flattern_layer = tf.keras.layers.Flatten()(conv_3)\n",
    "    flattern_layer = tf.keras.layers.Dense(start_neuron * 2,activation=tf.keras.layers.LeakyReLU())(flattern_layer)\n",
    "    \n",
    "#     output_0 = tf.keras.layers.Dense(32,activation='relu',)(flattern_layer)\n",
    "#     output_0 = tf.keras.layers.Dropout(dropout)(output_0)    \n",
    "    output_0 = tf.keras.layers.Dense(1,activation='linear',name='mean_rr')(flattern_layer)\n",
    "    \n",
    "    \n",
    "#     output_1 = tf.keras.layers.Dense(32,activation='relu',)(flattern_layer)\n",
    "#     output_1 = tf.keras.layers.Dropout(dropout)(output_1)    \n",
    "    output_1 = tf.keras.layers.Dense(1,activation='linear',name='area_t_to_p')(flattern_layer)\n",
    "    \n",
    "#     output_2 = tf.keras.layers.Dense(32,activation='relu',)(flattern_layer)\n",
    "#     output_2 = tf.keras.layers.Dropout(dropout)(output_2)    \n",
    "    output_2 = tf.keras.layers.Dense(1,activation='linear',name='area_p_to_t')(flattern_layer)\n",
    "    \n",
    "#     output_3 = tf.keras.layers.Dense(32,activation='relu',)(flattern_layer)\n",
    "#     output_3 = tf.keras.layers.Dropout(dropout)(output_3)   \n",
    "    output_3 = tf.keras.layers.Dense(1,activation='linear',name='time_t_to_p')(flattern_layer)\n",
    "    \n",
    "#     output_4 = tf.keras.layers.Dense(32,activation='relu',)(flattern_layer)    \n",
    "#     output_4 = tf.keras.layers.Dropout(dropout)(output_4)   \n",
    "    output_4 = tf.keras.layers.Dense(1,activation='linear',name='time_p_to_t')(flattern_layer)\n",
    "\n",
    "    \n",
    "    model = tf.keras.Model(input_layer, [output_0,output_1,output_2,output_3,output_4])\n",
    "    loss_weights = [scaling_factors[0]*5,scaling_factors[1],scaling_factors[2],scaling_factors[3],scaling_factors[4]]   \n",
    "    model.compile(optimizer = 'adam',loss = [tf.keras.losses.Huber(delta=delta),tf.keras.losses.Huber(delta=delta),tf.keras.losses.Huber(delta=delta),tf.keras.losses.Huber(delta=delta),tf.keras.losses.Huber(delta=delta)],loss_weights=loss_weights)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "792cef7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create model\n",
    "def create_CNN_LSTM_model_5_output(shape, start_neuron = 64, kernel_size=9, strides_size=1, max_pool_size=3, dropout=0.3, padding='valid', delta=1.0):    \n",
    "    input_layer = tf.keras.Input((shape[1], shape[2])) \n",
    "    \n",
    "    conv_1 = tf.keras.layers.Conv1D(start_neuron * 1, kernel_size, strides=strides_size,  padding=padding, activation=tf.keras.layers.LeakyReLU(),kernel_regularizer=tf.keras.regularizers.l2(0.01))(input_layer)\n",
    "    conv_1 = tf.keras.layers.MaxPool1D(max_pool_size,  padding=padding)(conv_1)\n",
    "    conv_1 = tf.keras.layers.Dropout(dropout)(conv_1)\n",
    "    \n",
    "    conv_2 = tf.keras.layers.Conv1D(start_neuron * 2, kernel_size, strides=strides_size, padding=padding, activation=tf.keras.layers.LeakyReLU(),kernel_regularizer=tf.keras.regularizers.l2(0.01))(conv_1)\n",
    "    conv_2 = tf.keras.layers.MaxPool1D(max_pool_size,  padding=padding)(conv_2)\n",
    "    conv_2 = tf.keras.layers.Dropout(dropout)(conv_2)    \n",
    "    \n",
    "    lstm_1 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(start_neuron, return_sequences=True))(conv_2)\n",
    "    lstm_1 = tf.keras.layers.Dropout(dropout)(lstm_1)\n",
    "\n",
    "    lstm_2 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(start_neuron * 2, return_sequences=True))(lstm_1)\n",
    "    lstm_2 = tf.keras.layers.Dropout(dropout)(lstm_2)\n",
    "    lstm_2 = tf.keras.layers.Flatten()(lstm_2)\n",
    "    \n",
    "    output_layer = tf.keras.layers.Dense(start_neuron*2,activation=tf.keras.layers.LeakyReLU())(lstm_2)\n",
    "    output_layer = tf.keras.layers.Dropout(dropout)(output_layer)\n",
    "    \n",
    "    #output_0 = tf.keras.layers.Dense(start_neuron*2,activation='relu')(output_layer)\n",
    "    output_0 = tf.keras.layers.Dense(1,activation='linear',name='mean_rr')(output_layer)\n",
    "\n",
    "    #output_1 = tf.keras.layers.Dense(start_neuron*2,activation='relu')(output_layer)\n",
    "    output_1 = tf.keras.layers.Dense(1,activation='linear',name='area_t_to_p')(output_layer)\n",
    "\n",
    "    #output_2 = tf.keras.layers.Dense(start_neuron*2,activation='relu')(output_layer)\n",
    "    output_2 = tf.keras.layers.Dense(1,activation='linear',name='area_p_to_t')(output_layer)\n",
    "\n",
    "    #output_3 = tf.keras.layers.Dense(start_neuron*2,activation='relu')(output_layer)\n",
    "    output_3 = tf.keras.layers.Dense(1,activation='linear',name='time_t_to_p')(output_layer)\n",
    "\n",
    "    #output_4 = tf.keras.layers.Dense(start_neuron*2,activation='relu')(output_layer)\n",
    "    output_4 = tf.keras.layers.Dense(1,activation='linear',name='time_p_to_t')(output_layer)\n",
    "\n",
    "    \n",
    "    model = tf.keras.Model(input_layer, [output_0,output_1,output_2,output_3,output_4])\n",
    "    loss_weights = [scaling_factors[0]*5,scaling_factors[1],scaling_factors[2],scaling_factors[3],scaling_factors[4]]   \n",
    "    #loss_weights = [scaling_factors[0]*5,0,0,0,0]  \n",
    "    model.compile(optimizer = 'adam',loss = [tf.keras.losses.Huber(delta=delta),tf.keras.losses.Huber(delta=delta),tf.keras.losses.Huber(delta=delta),tf.keras.losses.Huber(delta=delta),tf.keras.losses.Huber(delta=delta)],loss_weights=loss_weights)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "77614389-e95e-4ea4-9f72-9cf57f29facd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create model\n",
    "def create_time_wavelet_model_5_output(shape, start_neuron = 64, kernel_size=9, strides_size=1, max_pool_size=3, dropout=0.3, padding='valid', delta=1.0):    \n",
    "    \n",
    "    input_layer = tf.keras.Input((shape[1], shape[2]), name='input_time') \n",
    "    input_layer_wavelet = tf.keras.Input((shape[4], shape[2]), name='input_wavelet') \n",
    "\n",
    "    #time domain\n",
    "    conv_1 = tf.keras.layers.Conv1D(start_neuron * 1, kernel_size, strides=strides_size,  padding=padding, activation=tf.keras.layers.LeakyReLU(),kernel_regularizer=tf.keras.regularizers.l2(0.01))(input_layer)\n",
    "    conv_1 = tf.keras.layers.Conv1D(start_neuron * 1, kernel_size, strides=strides_size,  padding=padding, activation=tf.keras.layers.LeakyReLU(),kernel_regularizer=tf.keras.regularizers.l2(0.01))(conv_1)\n",
    "    conv_1 = tf.keras.layers.MaxPool1D(max_pool_size,  padding=padding)(conv_1)\n",
    "    conv_1 = tf.keras.layers.Dropout(dropout)(conv_1)\n",
    "    \n",
    "    conv_2 = tf.keras.layers.Conv1D(start_neuron * 2, kernel_size, strides=strides_size, padding=padding, activation=tf.keras.layers.LeakyReLU(),kernel_regularizer=tf.keras.regularizers.l2(0.01))(conv_1)\n",
    "    conv_2 = tf.keras.layers.Conv1D(start_neuron * 2, kernel_size, strides=strides_size, padding=padding, activation=tf.keras.layers.LeakyReLU(),kernel_regularizer=tf.keras.regularizers.l2(0.01))(conv_2)\n",
    "    conv_2 = tf.keras.layers.MaxPool1D(max_pool_size,  padding=padding)(conv_2)\n",
    "    conv_2 = tf.keras.layers.Dropout(dropout)(conv_2)    \n",
    "    \n",
    "    lstm_1 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(start_neuron, return_sequences=True))(conv_2)\n",
    "    lstm_1 = tf.keras.layers.Dropout(dropout)(lstm_1)\n",
    "\n",
    "    lstm_2 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(start_neuron * 2, return_sequences=True))(lstm_1)\n",
    "    lstm_2 = tf.keras.layers.Dropout(dropout)(lstm_2)\n",
    "    lstm_2 = tf.keras.layers.Flatten()(lstm_2)\n",
    "\n",
    "    #wavelet domain\n",
    "    conv_1_wavelet = tf.keras.layers.Conv1D(start_neuron * 1, kernel_size, strides=strides_size,  padding=padding, activation=tf.keras.layers.LeakyReLU(),kernel_regularizer=tf.keras.regularizers.l2(0.01))(input_layer_wavelet)\n",
    "    conv_1_wavelet = tf.keras.layers.MaxPool1D(max_pool_size,  padding=padding)(conv_1_wavelet)\n",
    "    conv_1_wavelet = tf.keras.layers.Dropout(dropout)(conv_1_wavelet)\n",
    "    \n",
    "    conv_2_wavelet = tf.keras.layers.Conv1D(start_neuron * 2, kernel_size, strides=strides_size, padding=padding, activation=tf.keras.layers.LeakyReLU(),kernel_regularizer=tf.keras.regularizers.l2(0.01))(conv_1_wavelet)\n",
    "    conv_2_wavelet = tf.keras.layers.MaxPool1D(max_pool_size,  padding=padding)(conv_2_wavelet)\n",
    "    conv_2_wavelet = tf.keras.layers.Dropout(dropout)(conv_2_wavelet)    \n",
    "    \n",
    "    lstm_1_wavelet = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(start_neuron, return_sequences=True))(conv_2_wavelet)\n",
    "    lstm_1_wavelet = tf.keras.layers.Dropout(dropout)(lstm_1_wavelet)\n",
    "\n",
    "    lstm_2_wavelet = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(start_neuron * 2, return_sequences=True))(lstm_1_wavelet)\n",
    "    lstm_2_wavelet = tf.keras.layers.Dropout(dropout)(lstm_2_wavelet)\n",
    "    lstm_2_wavelet = tf.keras.layers.Flatten()(lstm_2_wavelet)    \n",
    "\n",
    "    concat_layer = tf.keras.layers.Concatenate()([lstm_2, lstm_2_wavelet])\n",
    "    \n",
    "    output_layer = tf.keras.layers.Dense(start_neuron*2,activation=tf.keras.layers.LeakyReLU())(concat_layer)\n",
    "    output_layer = tf.keras.layers.Dropout(dropout)(output_layer)\n",
    "    \n",
    "    #output_0 = tf.keras.layers.Dense(start_neuron*2,activation='relu')(output_layer)\n",
    "    output_0 = tf.keras.layers.Dense(1,activation='linear',name='mean_rr')(output_layer)\n",
    "\n",
    "    #output_1 = tf.keras.layers.Dense(start_neuron*2,activation='relu')(output_layer)\n",
    "    output_1 = tf.keras.layers.Dense(1,activation='linear',name='area_t_to_p')(output_layer)\n",
    "\n",
    "    #output_2 = tf.keras.layers.Dense(start_neuron*2,activation='relu')(output_layer)\n",
    "    output_2 = tf.keras.layers.Dense(1,activation='linear',name='area_p_to_t')(output_layer)\n",
    "\n",
    "    #output_3 = tf.keras.layers.Dense(start_neuron*2,activation='relu')(output_layer)\n",
    "    output_3 = tf.keras.layers.Dense(1,activation='linear',name='time_t_to_p')(output_layer)\n",
    "\n",
    "    #output_4 = tf.keras.layers.Dense(start_neuron*2,activation='relu')(output_layer)\n",
    "    output_4 = tf.keras.layers.Dense(1,activation='linear',name='time_p_to_t')(output_layer)\n",
    "\n",
    "    \n",
    "    model = tf.keras.Model(inputs=[input_layer, input_layer_wavelet], outputs=[output_0,output_1,output_2,output_3,output_4])\n",
    "    loss_weights = [scaling_factors[0]*5,scaling_factors[1],scaling_factors[2],scaling_factors[3],scaling_factors[4]]   \n",
    "    #loss_weights = [scaling_factors[0]*5,0,0,0,0]  \n",
    "    model.compile(optimizer = 'adam',loss = [tf.keras.losses.Huber(delta=delta),tf.keras.losses.Huber(delta=delta),tf.keras.losses.Huber(delta=delta),tf.keras.losses.Huber(delta=delta),tf.keras.losses.Huber(delta=delta)],loss_weights=loss_weights)\n",
    "    #model.compile(optimizer = 'adam',loss = ['mae','mae','mae','mae','mae'],loss_weights=loss_weights)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ac3f69e8-9b4e-4d7e-ad5f-1bf3caf1b51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create model\n",
    "def custom_cnn_layers(input_layer, start_neuron, kernel_size, strides_size, max_pool_size, dropout, padding='valid', delta=1.0):\n",
    "    layer = tf.keras.layers.Conv1D(start_neuron, kernel_size, strides=strides_size,  padding=padding, activation=tf.keras.layers.LeakyReLU(),kernel_regularizer=tf.keras.regularizers.l2(0.01))(input_layer)\n",
    "    layer = tf.keras.layers.Conv1D(start_neuron, kernel_size, strides=strides_size,  padding=padding, activation=tf.keras.layers.LeakyReLU(),kernel_regularizer=tf.keras.regularizers.l2(0.01))(layer)\n",
    "    layer = tf.keras.layers.MaxPool1D(max_pool_size,  padding=padding)(layer)\n",
    "    layer = tf.keras.layers.Dropout(dropout)(layer)\n",
    "    return layer\n",
    "\n",
    "    \n",
    "def create_3_input_model_5_output(shape, start_neuron = 64, kernel_size=9, strides_size=1, max_pool_size=3, dropout=0.3, padding='valid', delta=1.0):\n",
    "    input_layer = tf.keras.Input((shape[1], shape[2]), name='input_time') \n",
    "    input_layer_wavelet = tf.keras.Input((shape[4], shape[2]), name='input_wavelet') \n",
    "    input_layer_feature = tf.keras.Input((shape[6],),name='input_feature')\n",
    "    \n",
    "    #time domain\n",
    "    conv_1 = custom_cnn_layers(input_layer, start_neuron = start_neuron, kernel_size = kernel_size, strides_size = strides_size, max_pool_size = 5, dropout = dropout, padding='same', delta=1.0)\n",
    "    conv_2 = custom_cnn_layers(conv_1, start_neuron = start_neuron * 2, kernel_size = kernel_size, strides_size = strides_size, max_pool_size = 5, dropout = dropout, padding='same', delta=1.0)\n",
    "    \n",
    "    conv_3 = custom_cnn_layers(conv_2, start_neuron = start_neuron * 4, kernel_size = kernel_size, strides_size = strides_size, max_pool_size = 3, dropout = dropout, padding='same', delta=1.0)\n",
    "    conv_4 = custom_cnn_layers(conv_3, start_neuron = start_neuron * 8, kernel_size = kernel_size, strides_size = strides_size, max_pool_size = 2, dropout = 0, padding='same', delta=1.0)\n",
    "    conv_4 = tf.keras.layers.GlobalAveragePooling1D()(conv_4)\n",
    "    conv_4 = tf.keras.layers.Dropout(dropout)(conv_4)\n",
    "\n",
    "    lstm_1 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(start_neuron * 2, return_sequences=True))(conv_2)\n",
    "    #lstm_1 = tf.keras.layers.MaxPool1D(3,  padding=padding)(lstm_1)\n",
    "    lstm_1 = tf.keras.layers.Dropout(dropout)(lstm_1)\n",
    "    \n",
    "    lstm_2 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(start_neuron * 4, return_sequences=True))(lstm_1)\n",
    "    lstm_2 = tf.keras.layers.GlobalAveragePooling1D()(lstm_2)\n",
    "    lstm_2 = tf.keras.layers.Dropout(dropout)(lstm_2)\n",
    "\n",
    "    #wavelet domain\n",
    "    conv_1_wavelet = custom_cnn_layers(input_layer_wavelet, start_neuron = start_neuron, kernel_size = kernel_size, strides_size = strides_size, max_pool_size = 5, dropout = dropout, padding='same', delta=1.0)\n",
    "    conv_2_wavelet = custom_cnn_layers(conv_1_wavelet, start_neuron = start_neuron * 2, kernel_size = kernel_size, strides_size = strides_size, max_pool_size = 5, dropout = dropout, padding='same', delta=1.0)\n",
    "    \n",
    "    conv_3_wavelet = custom_cnn_layers(conv_2_wavelet, start_neuron = start_neuron * 4, kernel_size = kernel_size, strides_size = strides_size, max_pool_size = 3, dropout = dropout, padding='same', delta=1.0)\n",
    "    conv_4_wavelet = custom_cnn_layers(conv_3_wavelet, start_neuron = start_neuron * 8, kernel_size = kernel_size, strides_size = strides_size, max_pool_size = 2, dropout = 0, padding='same', delta=1.0)\n",
    "    conv_4_wavelet = tf.keras.layers.GlobalAveragePooling1D()(conv_4_wavelet)\n",
    "    conv_4_wavelet = tf.keras.layers.Dropout(dropout)(conv_4_wavelet)\n",
    "\n",
    "    lstm_1_wavelet = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(start_neuron * 2, return_sequences=True))(conv_2_wavelet)\n",
    "    #lstm_1_wavelet = tf.keras.layers.MaxPool1D(3,  padding=padding)(lstm_1_wavelet)\n",
    "    lstm_1_wavelet = tf.keras.layers.Dropout(dropout)(lstm_1_wavelet)\n",
    "    \n",
    "    lstm_2_wavelet = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(start_neuron * 4, return_sequences=True))(lstm_1_wavelet)\n",
    "    lstm_2_wavelet = tf.keras.layers.GlobalAveragePooling1D()(lstm_2_wavelet)\n",
    "    lstm_2_wavelet = tf.keras.layers.Dropout(dropout)(lstm_2_wavelet)\n",
    "\n",
    "    \n",
    "    concat_layer = tf.keras.layers.Concatenate()([conv_4,lstm_2,conv_4_wavelet,lstm_2_wavelet,input_layer_feature])\n",
    "    output_layer = tf.keras.layers.Dense(start_neuron*8,activation=tf.keras.layers.LeakyReLU())(concat_layer)\n",
    "    output_layer = tf.keras.layers.Dropout(dropout)(output_layer)\n",
    "    \n",
    "    #output_0 = tf.keras.layers.Dense(start_neuron*2,activation='relu')(output_layer)\n",
    "    output_0 = tf.keras.layers.Dense(1,activation='linear',name='mean_rr')(output_layer)\n",
    "\n",
    "    #output_1 = tf.keras.layers.Dense(start_neuron*2,activation='relu')(output_layer)\n",
    "    output_1 = tf.keras.layers.Dense(1,activation='linear',name='area_t_to_p')(output_layer)\n",
    "\n",
    "    #output_2 = tf.keras.layers.Dense(start_neuron*2,activation='relu')(output_layer)\n",
    "    output_2 = tf.keras.layers.Dense(1,activation='linear',name='area_p_to_t')(output_layer)\n",
    "\n",
    "    #output_3 = tf.keras.layers.Dense(start_neuron*2,activation='relu')(output_layer)\n",
    "    output_3 = tf.keras.layers.Dense(1,activation='linear',name='time_t_to_p')(output_layer)\n",
    "\n",
    "    #output_4 = tf.keras.layers.Dense(start_neuron*2,activation='relu')(output_layer)\n",
    "    output_4 = tf.keras.layers.Dense(1,activation='linear',name='time_p_to_t')(output_layer)\n",
    "\n",
    "    \n",
    "    model = tf.keras.Model(inputs=[input_layer, input_layer_wavelet, input_layer_feature], outputs=[output_0,output_1,output_2,output_3,output_4])\n",
    "    loss_weights = [scaling_factors[0]*5,scaling_factors[1],scaling_factors[2],scaling_factors[3],scaling_factors[4]]   \n",
    "    #loss_weights = [scaling_factors[0]*5,0,0,0,0]  \n",
    "    model.compile(optimizer = 'adam',loss = [tf.keras.losses.Huber(delta=delta),tf.keras.losses.Huber(delta=delta),tf.keras.losses.Huber(delta=delta),tf.keras.losses.Huber(delta=delta),tf.keras.losses.Huber(delta=delta)],loss_weights=loss_weights)\n",
    "    #model.compile(optimizer = 'adam',loss = ['mae','mae','mae','mae','mae'],loss_weights=loss_weights)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4c775e28-1e6b-4cc7-a039-2f53af66a9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_LR_On_Plateau = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',factor=0.5,patience=PATIENCE//2,verbose=1, min_delta=0.00001,)\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=PATIENCE, restore_best_weights = True, mode = 'min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b79be33c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1760991182.250356   11042 gpu_device.cc:2430] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 12.0. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.\n",
      "I0000 00:00:1760991182.250517   11042 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13121 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 5070 Ti, pci bus id: 0000:01:00.0, compute capability: 12.0\n",
      "2025-10-20 16:13:02.895260: W tensorflow/compiler/mlir/tools/kernel_gen/tf_gpu_runtime_wrappers.cc:40] 'cuModuleLoadData(&module, data)' failed with 'CUDA_ERROR_INVALID_PTX'\n",
      "\n",
      "2025-10-20 16:13:02.895311: W tensorflow/compiler/mlir/tools/kernel_gen/tf_gpu_runtime_wrappers.cc:40] 'cuModuleGetFunction(&function, module, kernel_name)' failed with 'CUDA_ERROR_INVALID_HANDLE'\n",
      "\n",
      "2025-10-20 16:13:02.895324: W tensorflow/core/framework/op_kernel.cc:1844] INTERNAL: 'cuLaunchKernel(function, gridX, gridY, gridZ, blockX, blockY, blockZ, 0, reinterpret_cast<CUstream>(stream), params, nullptr)' failed with 'CUDA_ERROR_INVALID_HANDLE'\n",
      "2025-10-20 16:13:02.895343: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: INTERNAL: 'cuLaunchKernel(function, gridX, gridY, gridZ, blockX, blockY, blockZ, 0, reinterpret_cast<CUstream>(stream), params, nullptr)' failed with 'CUDA_ERROR_INVALID_HANDLE'\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "{{function_node __wrapped__Cast_device_/job:localhost/replica:0/task:0/device:GPU:0}} 'cuLaunchKernel(function, gridX, gridY, gridZ, blockX, blockY, blockZ, 0, reinterpret_cast<CUstream>(stream), params, nullptr)' failed with 'CUDA_ERROR_INVALID_HANDLE' [Op:Cast] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInternalError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m model = \u001b[43mcreate_3_input_model_5_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_wavelet\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_feature\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_neuron\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m11\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrides_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m                                  \u001b[49m\u001b[43mmax_pool_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msame\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.75\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m model.summary()\n\u001b[32m      5\u001b[39m history = model.fit([X_train,X_train_wavelet,X_train_feature], [Y_train[:,\u001b[32m0\u001b[39m],Y_train[:,\u001b[32m1\u001b[39m],Y_train[:,\u001b[32m2\u001b[39m],Y_train[:,\u001b[32m3\u001b[39m],Y_train[:,\u001b[32m4\u001b[39m]], \n\u001b[32m      6\u001b[39m                     validation_data=([X_val,X_val_wavelet,X_val_feature], [Y_val[:,\u001b[32m0\u001b[39m],Y_val[:,\u001b[32m1\u001b[39m],Y_val[:,\u001b[32m2\u001b[39m],Y_val[:,\u001b[32m3\u001b[39m],Y_val[:,\u001b[32m4\u001b[39m]]), \n\u001b[32m      7\u001b[39m                     epochs=EPOCHS, batch_size=BATCH_SIZE,verbose=\u001b[32m1\u001b[39m, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m, callbacks=[reduce_LR_On_Plateau,early_stopping])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mcreate_3_input_model_5_output\u001b[39m\u001b[34m(shape, start_neuron, kernel_size, strides_size, max_pool_size, dropout, padding, delta)\u001b[39m\n\u001b[32m     13\u001b[39m input_layer_feature = tf.keras.Input((shape[\u001b[32m6\u001b[39m],),name=\u001b[33m'\u001b[39m\u001b[33minput_feature\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m#time domain\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m conv_1 = \u001b[43mcustom_cnn_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_neuron\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_neuron\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrides_size\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrides_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_pool_size\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msame\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m conv_2 = custom_cnn_layers(conv_1, start_neuron = start_neuron * \u001b[32m2\u001b[39m, kernel_size = kernel_size, strides_size = strides_size, max_pool_size = \u001b[32m5\u001b[39m, dropout = dropout, padding=\u001b[33m'\u001b[39m\u001b[33msame\u001b[39m\u001b[33m'\u001b[39m, delta=\u001b[32m1.0\u001b[39m)\n\u001b[32m     19\u001b[39m conv_3 = custom_cnn_layers(conv_2, start_neuron = start_neuron * \u001b[32m4\u001b[39m, kernel_size = kernel_size, strides_size = strides_size, max_pool_size = \u001b[32m3\u001b[39m, dropout = dropout, padding=\u001b[33m'\u001b[39m\u001b[33msame\u001b[39m\u001b[33m'\u001b[39m, delta=\u001b[32m1.0\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mcustom_cnn_layers\u001b[39m\u001b[34m(input_layer, start_neuron, kernel_size, strides_size, max_pool_size, dropout, padding, delta)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcustom_cnn_layers\u001b[39m(input_layer, start_neuron, kernel_size, strides_size, max_pool_size, dropout, padding=\u001b[33m'\u001b[39m\u001b[33mvalid\u001b[39m\u001b[33m'\u001b[39m, delta=\u001b[32m1.0\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     layer = \u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkeras\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlayers\u001b[49m\u001b[43m.\u001b[49m\u001b[43mConv1D\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_neuron\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrides\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstrides_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkeras\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlayers\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLeakyReLU\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mkernel_regularizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkeras\u001b[49m\u001b[43m.\u001b[49m\u001b[43mregularizers\u001b[49m\u001b[43m.\u001b[49m\u001b[43ml2\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_layer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m     layer = tf.keras.layers.Conv1D(start_neuron, kernel_size, strides=strides_size,  padding=padding, activation=tf.keras.layers.LeakyReLU(),kernel_regularizer=tf.keras.regularizers.l2(\u001b[32m0.01\u001b[39m))(layer)\n\u001b[32m      5\u001b[39m     layer = tf.keras.layers.MaxPool1D(max_pool_size,  padding=padding)(layer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tf/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tf/lib/python3.12/site-packages/keras/src/backend/tensorflow/core.py:152\u001b[39m, in \u001b[36mconvert_to_tensor\u001b[39m\u001b[34m(x, dtype, sparse, ragged)\u001b[39m\n\u001b[32m    147\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype == \u001b[33m\"\u001b[39m\u001b[33mbool\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_int_dtype(dtype):\n\u001b[32m    148\u001b[39m         \u001b[38;5;66;03m# TensorFlow conversion is stricter than other backends, it does not\u001b[39;00m\n\u001b[32m    149\u001b[39m         \u001b[38;5;66;03m# allow ints for bools or floats for ints. We convert without dtype\u001b[39;00m\n\u001b[32m    150\u001b[39m         \u001b[38;5;66;03m# and cast instead.\u001b[39;00m\n\u001b[32m    151\u001b[39m         x = tf.convert_to_tensor(x)\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    153\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tf.convert_to_tensor(x, dtype=dtype)\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m standardize_dtype(x.dtype) == dtype:\n",
      "\u001b[31mInternalError\u001b[39m: {{function_node __wrapped__Cast_device_/job:localhost/replica:0/task:0/device:GPU:0}} 'cuLaunchKernel(function, gridX, gridY, gridZ, blockX, blockY, blockZ, 0, reinterpret_cast<CUstream>(stream), params, nullptr)' failed with 'CUDA_ERROR_INVALID_HANDLE' [Op:Cast] name: "
     ]
    }
   ],
   "source": [
    "model = create_3_input_model_5_output((X_train.shape + X_train_wavelet.shape + X_train_feature.shape), start_neuron = 32, kernel_size= 11, strides_size=1, \n",
    "                                  max_pool_size= 5, dropout=0.3,padding='same', delta=0.75)\n",
    "model.summary()\n",
    "\n",
    "history = model.fit([X_train,X_train_wavelet,X_train_feature], [Y_train[:,0],Y_train[:,1],Y_train[:,2],Y_train[:,3],Y_train[:,4]], \n",
    "                    validation_data=([X_val,X_val_wavelet,X_val_feature], [Y_val[:,0],Y_val[:,1],Y_val[:,2],Y_val[:,3],Y_val[:,4]]), \n",
    "                    epochs=EPOCHS, batch_size=BATCH_SIZE,verbose=1, shuffle=True, callbacks=[reduce_LR_On_Plateau,early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62753818",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a0e88ed",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c40d746",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model.predict([X_test,X_test_wavelet,X_test_feature],verbose = 1)\n",
    "Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80933e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation loss values\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "# # Plot training & validation MAE values\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.plot(history.history['mae'], label='Training MAE')\n",
    "# plt.plot(history.history['val_mae'], label='Validation MAE')\n",
    "# plt.title('Model MAE')\n",
    "# plt.ylabel('MAE')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.legend(loc='upper right')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f06927",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86891ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7d2fd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c26d7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fit and plot piecewise linear regression\n",
    "def piecewise_regression(x, y, breakpoints, ax):\n",
    "    segments = np.split(np.argsort(x), np.searchsorted(x[np.argsort(x)], breakpoints))\n",
    "    segment_texts = []\n",
    "    \n",
    "    for i,segment in enumerate(segments):\n",
    "        if len(segment) > 1:  # Fit line if segment contains more than one point\n",
    "            slope, intercept, _, _, _ = linregress(x[segment], y[segment])\n",
    "            ax.plot(x[segment], slope * x[segment] + intercept, color='red', linewidth=2)\n",
    "            \n",
    "            # Calculate Pearson correlation coefficient and p-value for each segment\n",
    "            corr, p_val = pearsonr(x[segment], y[segment])\n",
    "            spearman_corr, spearman_p_val = spearmanr(x[segment], y[segment])\n",
    "            segment_texts.append(f'Segment {i}: Pearsonr: {corr:.2f},  Spearman: {spearman_corr:.2f}')\n",
    "    \n",
    "    return segment_texts\n",
    "# Get a list of unique subject IDs and create a color map\n",
    "unique_subject_ids = np.unique(test_subject_id)\n",
    "colors = plt.colormaps['tab20'](np.linspace(0, 1, len(unique_subject_ids)))\n",
    "color_map = {subject_id: colors[i] for i, subject_id in enumerate(unique_subject_ids)}\n",
    "\n",
    "# Initialize the figure and subplots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "output_names = ['Mean Respiration Rate', 'Area T to P', 'Area P to T', 'Time T to P', 'Time P to T']\n",
    "\n",
    "for i in range(5):\n",
    "    # Calculate the Pearson correlation coefficient\n",
    "    correlation, p_value = pearsonr(Y_test[:, i], Y_pred[i].flatten())\n",
    "    spearman_corr, spearman_p_value = spearmanr(Y_test[:, i], Y_pred[i].flatten())\n",
    "    print(f\"Output {i}: Pearson Correlation: {correlation}\")\n",
    "    print(f\"Output {i}: P-value: {p_value}\")\n",
    "\n",
    "    # # Create scatter plot\n",
    "    # axes[i].scatter(Y_test[:, i], Y_pred[i].flatten(), alpha=0.6, edgecolors='w', linewidth=0.5, label='Data points')\n",
    "    \n",
    "    for subject_id in unique_subject_ids:\n",
    "        mask = test_subject_id == subject_id\n",
    "        axes[i].scatter(Y_test[mask, i], Y_pred[i].flatten()[mask], alpha=0.6, edgecolors='w', linewidth=0.5, label=f'Subject {subject_id}', color=color_map[subject_id])\n",
    "    # Define breakpoints\n",
    "    breakpoints = np.percentile(Y_test[:, i], [33, 66])\n",
    "\n",
    "    # Fit and plot piecewise regression\n",
    "    segment_texts = piecewise_regression(Y_test[:, i], Y_pred[i].flatten(), breakpoints, axes[i])\n",
    "\n",
    "    axes[i].set_title(f'Actual vs Predicted {output_names[i]}')\n",
    "    axes[i].set_xlabel('Actual')\n",
    "    axes[i].set_ylabel('Predicted')\n",
    "    axes[i].grid(True)\n",
    "    axes[i].legend(loc='upper right')\n",
    "\n",
    "    # Add text box with correlation and p-value\n",
    "    textstr = f'Pearson Correlation: {correlation:.2f}, Spearman: {spearman_corr:.2f}\\n' + '\\n'.join(segment_texts)\n",
    "    props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "    axes[i].text(0.05, 0.95, textstr, transform=axes[i].transAxes, fontsize=10,\n",
    "                 verticalalignment='top', bbox=props)\n",
    "\n",
    "# Plot the model loss\n",
    "axes[5].plot(history.history['loss'], label='Training Loss')\n",
    "axes[5].plot(history.history['val_loss'], label='Validation Loss')\n",
    "axes[5].set_title('Model Loss')\n",
    "axes[5].set_ylabel('Loss')\n",
    "axes[5].set_xlabel('Epoch')\n",
    "axes[5].legend()\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Ensure x and y axes have the same scale\n",
    "for i, ax in enumerate(axes):\n",
    "    if i < len(output_names):\n",
    "        ax.set_xlim([min(Y_test[:, i].min(), Y_pred[i].min()), max(Y_test[:, i].max(), Y_pred[i].max())])\n",
    "        ax.set_ylim([min(Y_test[:, i].min(), Y_pred[i].min()), max(Y_test[:, i].max(), Y_pred[i].max())])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ecadc7-0b90-44b5-bec5-791af6743ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(test_subject_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac88b75-94ad-4f06-a059-378e84492611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_id = ['f2y08']\n",
    "# list_id = np.unique(test_subject_id)\n",
    "# num_examples_to_plot = 10\n",
    "\n",
    "# for current_subject_id in np.unique(test_subject_id):\n",
    "    \n",
    "#     # Shuffle and select a few examples from the test set\n",
    "#     mask = (test_subject_id == current_subject_id)\n",
    "#     Y_pred_filtered_rr = Y_pred[0][mask].flatten() \n",
    "#     Y_pred_filtered_area_T_to_P = Y_pred[1][mask].flatten()\n",
    "#     Y_pred_filtered_area_P_to_T = Y_pred[2][mask].flatten()\n",
    "#     Y_pred_filtered_time_T_to_P = Y_pred[3][mask].flatten()\n",
    "#     Y_pred_filtered_time_P_to_T = Y_pred[4][mask].flatten()\n",
    "    \n",
    "#     Y_true_filtered_rr = Y_test[mask][:,0]\n",
    "#     Y_true_filtered_area_T_to_P = Y_test[mask][:,1]\n",
    "#     Y_true_filtered_area_P_to_T = Y_test[mask][:,2]\n",
    "#     Y_true_filtered_time_T_to_P = Y_test[mask][:,3]\n",
    "#     Y_true_filtered_time_P_to_T = Y_test[mask][:,4]\n",
    "    \n",
    "#     X_test_filtered = X_test[mask]\n",
    "    \n",
    "    \n",
    "    \n",
    "#     #Plot the selected examples from the test set\n",
    "#     fig, axes = plt.subplots(num_examples_to_plot, 1, figsize=(12, 3*num_examples_to_plot))\n",
    "#     for i, ax in enumerate(axes):\n",
    "#         idx = random.randint(0, len(X_test_filtered)-1)\n",
    "#         print(idx)\n",
    "#         # Plot the sample\n",
    "#         ax.plot(X_test_filtered[idx].flatten())\n",
    "#         ax.set_title(f'Subject ID: {current_subject_id}. Test Sample: {idx}')\n",
    "    \n",
    "#         #Adding text box for comparison\n",
    "#         textstr = '\\n'.join((\n",
    "#             f'             [Respiration Rate, Area Trough to Peak, Area Peak to Trough, Time Trough to Peak, Time Peak to Trough]', \n",
    "#             f'Predict: [{Y_pred_filtered_rr[idx]:.2f},  {Y_pred_filtered_area_T_to_P[idx]:22.2f},  {Y_pred_filtered_area_P_to_T[idx]:27.2f},  {Y_pred_filtered_time_T_to_P[idx]:27.2f},  {Y_pred_filtered_time_P_to_T[idx]:27.2f}]',\n",
    "#             f'True:     [{Y_true_filtered_rr[idx]:.2f},  {Y_true_filtered_area_T_to_P[idx]:22.2f},  {Y_true_filtered_area_P_to_T[idx]:27.2f},  {Y_true_filtered_time_T_to_P[idx]:27.2f},  {Y_true_filtered_time_P_to_T[idx]:27.2f}]',\n",
    "#         ))\n",
    "#         ax.text(0.05, 0.95, textstr, transform=ax.transAxes, fontsize=10,\n",
    "#                 verticalalignment='top', bbox=dict(facecolor='yellow', alpha=0.8))\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "    \n",
    "#     SAVE_DIR = OUTPUT_DIR + 'sample/'\n",
    "#     if(not(os.path.exists(SAVE_DIR))):\n",
    "#         os.makedirs(SAVE_DIR)\n",
    "#     plt.savefig(SAVE_DIR + f'sample_{current_subject_id}.png')\n",
    "#     #plt.show()\n",
    "#     plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30a0c0e-026a-4ffe-a21c-725ba13fb016",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ca35ed-8718-41f0-842d-a6df3b2c9a6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee22f5c-26f9-47dc-a2ca-d5b0a8443c37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6d3bbf-9263-41e9-b29d-708024fb9370",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "204328d7",
   "metadata": {},
   "source": [
    "# Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f299737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "# from scipy.stats import pearsonr, linregress\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def piecewise_regression(x, y, breakpoints, ax):\n",
    "#     segments = np.split(np.argsort(x), np.searchsorted(x[np.argsort(x)], breakpoints))\n",
    "#     segment_texts = []\n",
    "    \n",
    "#     for i,segment in enumerate(segments):\n",
    "#         if len(segment) > 1:  # Fit line if segment contains more than one point\n",
    "#             slope, intercept, _, _, _ = linregress(x[segment], y[segment])\n",
    "#             ax.plot(x[segment], slope * x[segment] + intercept, color='red', linewidth=2)\n",
    "            \n",
    "#             # Calculate Pearson correlation coefficient and p-value for each segment\n",
    "#             corr, p_val = pearsonr(x[segment], y[segment])\n",
    "#             spearman_corr, spearman_p_val = spearmanr(x[segment], y[segment])\n",
    "#             segment_texts.append(f'Segment {i}: Pearsonr: {corr:.2f},  Spearman: {spearman_corr:.2f}')\n",
    "    \n",
    "#     return segment_texts\n",
    "    \n",
    "# if(not(os.path.exists(OUTPUT_DIR))):\n",
    "#     os.makedirs(OUTPUT_DIR)\n",
    "    \n",
    "# # Define the grid search parameters\n",
    "# param_grid = {\n",
    "#     'start_neuron': [32],\n",
    "#     'kernel_size': [9],\n",
    "#     'strides_size': [3],\n",
    "#     'max_pool_size': [5],\n",
    "#     'dropout': [0.3],\n",
    "#     'padding' :['same'],\n",
    "#     'batch_size' : [64],\n",
    "#     'huber_delta' : [0.75],\n",
    "# }\n",
    "\n",
    "# # Prepare callbacks\n",
    "\n",
    "\n",
    "# # Number of epochs\n",
    "# EPOCHS = 200\n",
    "# PATIENCE = 20\n",
    "\n",
    "# # Initialize a list to store the loss history for all configurations\n",
    "# loss_history = []\n",
    "\n",
    "# # Perform grid search\n",
    "# for start_neuron in param_grid['start_neuron']:\n",
    "#     for kernel_size in param_grid['kernel_size']:\n",
    "#         for strides_size in param_grid['strides_size']:\n",
    "#             for max_pool_size in param_grid['max_pool_size']:\n",
    "#                 for dropout in param_grid['dropout']:\n",
    "#                     for padding in param_grid['padding']:\n",
    "#                         for batch_size in param_grid['batch_size']:\n",
    "#                             for delta in param_grid['huber_delta']:\n",
    "#                                 print(f'start_neuron={start_neuron}, kernel_size={kernel_size}, strides_size={strides_size}, max_pool_size={max_pool_size}, dropout={dropout}, padding={padding}, batch_size={batch_size}, huber_delta={delta}')\n",
    "                                \n",
    "#                                 reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',factor=0.5,patience=PATIENCE//2,verbose=1, min_delta=0.00001,)\n",
    "#                                 early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=PATIENCE, restore_best_weights = True, mode = 'min')\n",
    "#                                 # Create the model\n",
    "#                                 model = create_3_input_model_5_output((X_train.shape + X_train_wavelet.shape + X_train_feature.shape), start_neuron=start_neuron, kernel_size=kernel_size,\n",
    "#                                                                   strides_size=strides_size, max_pool_size=max_pool_size, \n",
    "#                                                                   dropout=dropout, padding=padding, delta=delta)\n",
    "#                                 #model.summary()\n",
    "    \n",
    "    \n",
    "#                                 # Train the model\n",
    "#                                 history = model.fit([X_train,X_train_wavelet,X_train_feature], [Y_train[:,0],Y_train[:,1],Y_train[:,2],Y_train[:,3],Y_train[:,4]], \n",
    "#                                                     validation_data=([X_val,X_val_wavelet,X_val_feature], [Y_val[:,0],Y_val[:,1],Y_val[:,2],Y_val[:,3],Y_val[:,4]]), \n",
    "#                                                     epochs=EPOCHS, batch_size=BATCH_SIZE,verbose=1, shuffle=True, \n",
    "#                                                     callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "\n",
    "\n",
    "#                                 min_train_loss = min(history.history['loss'])\n",
    "#                                 min_val_lost = min(history.history['val_loss'])\n",
    "#                                 # Save loss history for the current configuration\n",
    "#                                 config_loss_history = {\n",
    "#                                     'config': f'start_neuron={start_neuron}, kernel_size={kernel_size}, strides_size={strides_size}, max_pool_size={max_pool_size}, dropout={dropout}, padding={padding}, batch_size={batch_size}, huber_delta={delta}',\n",
    "#                                     'training_loss': history.history['loss'],\n",
    "#                                     'validation_loss': history.history['val_loss'],\n",
    "#                                     'min' : f'Best_train_loss{min_train_loss}, best_val_loss{min_val_lost}'\n",
    "#                                 }\n",
    "#                                 loss_history.append(config_loss_history)\n",
    "    \n",
    "#                                 # Evaluate the model\n",
    "#                                 Y_pred = model.predict([X_test,X_test_wavelet,X_test_feature],verbose=0)\n",
    "\n",
    "#                                 plt.clf()\n",
    "#                                 unique_subject_ids = np.unique(test_subject_id)\n",
    "#                                 colors = plt.colormaps['tab20'](np.linspace(0, 1, len(unique_subject_ids)))\n",
    "#                                 color_map = {subject_id: colors[i] for i, subject_id in enumerate(unique_subject_ids)}\n",
    "\n",
    "\n",
    "#                                 # Initialize the figure and subplots\n",
    "#                                 # Create plots\n",
    "#                                 fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "#                                 axes = axes.flatten()\n",
    "#                                 output_names = ['Mean Respiration Rate', 'Area T to P', 'Area P to T', 'Time T to P', 'Time P to T']\n",
    "    \n",
    "#                                 for i in range(5):\n",
    "#                                     # Calculate the Pearson correlation coefficient\n",
    "#                                     correlation, p_value = pearsonr(Y_test[:, i], Y_pred[i].flatten())\n",
    "#                                     print(f\"Output {i}: Pearson Correlation: {correlation}\")\n",
    "#                                     print(f\"Output {i}: P-value: {p_value}\")\n",
    "                                \n",
    "#                                     # # Create scatter plot\n",
    "#                                     # axes[i].scatter(Y_test[:, i], Y_pred[i].flatten(), alpha=0.6, edgecolors='w', linewidth=0.5, label='Data points')\n",
    "                                    \n",
    "#                                     for subject_id in unique_subject_ids:\n",
    "#                                         mask = test_subject_id == subject_id\n",
    "#                                         axes[i].scatter(Y_test[mask, i], Y_pred[i].flatten()[mask], alpha=0.6, edgecolors='w', linewidth=0.5, label=f'Subject {subject_id}', color=color_map[subject_id])\n",
    "                                \n",
    "#                                     # Define breakpoints\n",
    "#                                     breakpoints = np.percentile(Y_test[:, i], [33, 66])\n",
    "                                \n",
    "#                                     # Fit and plot piecewise regression\n",
    "#                                     segment_texts = piecewise_regression(Y_test[:, i], Y_pred[i].flatten(), breakpoints, axes[i])\n",
    "                                \n",
    "#                                     axes[i].set_title(f'Actual vs Predicted {output_names[i]}')\n",
    "#                                     axes[i].set_xlabel('Actual')\n",
    "#                                     axes[i].set_ylabel('Predicted')\n",
    "#                                     axes[i].grid(True)\n",
    "#                                     axes[i].legend()\n",
    "                                \n",
    "#                                     # Add text box with correlation and p-value\n",
    "#                                     textstr = f'Pearson Correlation: {correlation:.2f}\\n' + '\\n'.join(segment_texts)\n",
    "#                                     props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "#                                     axes[i].text(0.05, 0.95, textstr, transform=axes[i].transAxes, fontsize=10,\n",
    "#                                                  verticalalignment='top', bbox=props)\n",
    "                                \n",
    "#                                 # Plot the model loss\n",
    "#                                 axes[5].plot(history.history['loss'], label='Training Loss')\n",
    "#                                 axes[5].plot(history.history['val_loss'], label='Validation Loss')\n",
    "#                                 axes[5].set_title('Model Loss')\n",
    "#                                 axes[5].set_ylabel('Loss')\n",
    "#                                 axes[5].set_xlabel('Epoch')\n",
    "#                                 axes[5].legend(loc='upper right')\n",
    "\n",
    "#                                 # Adjust layout to prevent overlap\n",
    "#                                 plt.tight_layout()\n",
    "                                \n",
    "#                                 # Ensure x and y axes have the same scale\n",
    "#                                 for i, ax in enumerate(axes):\n",
    "#                                     if i < len(output_names):\n",
    "#                                         ax.set_xlim([min(Y_test[:,i].min(), Y_pred[i].min()), max(Y_test[:,i].max(), Y_pred[i].max())])\n",
    "#                                         ax.set_ylim([min(Y_test[:,i].min(), Y_pred[i].min()), max(Y_test[:,i].max(), Y_pred[i].max())])\n",
    "    \n",
    "#                                 # Save the plot\n",
    "#                                 plt.savefig(OUTPUT_DIR + f'grid_search_{start_neuron}_{kernel_size}_{strides_size}_{max_pool_size}_{dropout}_{padding}_{batch_size}_{delta}.png')\n",
    "#                                 plt.close()\n",
    "\n",
    "# # Save loss history to a text file\n",
    "# with open(OUTPUT_DIR + 'loss_history.txt', 'w') as f:\n",
    "#     for entry in loss_history:\n",
    "#         f.write(f\"Configuration: {entry['config']}\\n\")\n",
    "#         f.write(f\"Training Loss: {entry['training_loss']}\\n\")\n",
    "#         f.write(f\"Validation Loss: {entry['validation_loss']}\\n\")\n",
    "#         f.write(f\"{entry['min']}\\n\")\n",
    "#         f.write(\"\\n\")\n",
    "\n",
    "# print(\"Grid search completed and results saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7012cfe3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c44e1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid = {\n",
    "#     'start_neuron': [32,64],\n",
    "#     'kernel_size': [5,7,9,11],\n",
    "#     'strides_size': [1,2,3,4],\n",
    "#     'max_pool_size': [2,3],\n",
    "#     'dropout': [0.3,0.4,0.5],\n",
    "#     'padding' :['same','valid'],\n",
    "#     'batch_size' : [64],\n",
    "#     'huber_delta' : [0.5,1],\n",
    "    \n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd0253e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df6df10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11fe6e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96903c4-e0f6-4efd-8c18-c8214c17334d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7c76df-2cda-4828-8de1-18318984c8a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90b4564-a4a4-4b90-9559-32be669d5798",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801aad75-4d8e-4817-921f-074e56385757",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ad6ee4-3356-4658-a2a2-b1aea82e0481",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
